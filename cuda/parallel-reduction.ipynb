{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "153ffb63-fa1f-4d9a-a8b0-cbc07a60512b",
   "metadata": {},
   "source": [
    "# Parallel reduction\n",
    "\n",
    "[Optimising parallel reduction in CUDA](https://developer.download.nvidia.com/assets/cuda/files/reduction.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1b425f-3f99-4c2d-8c88-f60a784930e5",
   "metadata": {},
   "source": [
    "## Reduce bond forces to particle forces\n",
    "\n",
    "Bond forces can be stored as a bondlist or neighbour list\n",
    "\n",
    "- bondlist [n_bonds, 2]\n",
    "- neighbourlist [n_particles, n_family_members]\n",
    "\n",
    "Reduce:\n",
    "- particles.forces [n_particles, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0edfd766-e9df-4e97-8632-19e5b1ad433a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from functools import wraps\n",
    "\n",
    "import numpy as np\n",
    "from numba import njit, prange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a0db2d6-e56e-4450-af91-1a3faf316ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeit(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end = time.time()\n",
    "        print(f\"Function '{func.__name__}' executed in {end - start:.4f} seconds\")\n",
    "        return result\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35899922-5ebf-4930-921d-4460e36c3286",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "n_particles = 1000000\n",
    "n_family_members = 200\n",
    "\n",
    "neighbourlist = np.random.rand(n_particles, n_family_members)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4138f7-44d0-4a81-9d4a-1add7822b4f6",
   "metadata": {},
   "source": [
    "### Numpy and Numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "782a0d56-cfd3-4d51-98f1-5566c943cdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit\n",
    "def reduce_bond_forces_a(neighbourlist):\n",
    "    n_particles = neighbourlist.shape[0]\n",
    "    f = np.zeros((n_particles)) \n",
    "    for i in range(n_particles):\n",
    "        f[i] = np.sum(neighbourlist[i, :])\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6d2f151-f9c4-4afb-9c9c-678c3e136ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit\n",
    "def reduce_bond_forces_b(neighbourlist):\n",
    "    return np.sum(neighbourlist, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25f1cec9-a860-44a3-99a2-c8a74d74b6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit\n",
    "@njit(parallel=True, fastmath=True)\n",
    "def reduce_bond_forces_c(neighbourlist):\n",
    "    n_particles = neighbourlist.shape[0]\n",
    "    f = np.zeros((n_particles)) \n",
    "    for i in prange(n_particles):\n",
    "        f[i] = np.sum(neighbourlist[i, :])\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "131c4a64-9cd1-426d-811d-791b9f78f8f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'reduce_bond_forces_a' executed in 1.3762 seconds\n",
      "Function 'reduce_bond_forces_b' executed in 0.0404 seconds\n",
      "Function 'reduce_bond_forces_c' executed in 0.3432 seconds\n"
     ]
    }
   ],
   "source": [
    "f = reduce_bond_forces_a(neighbourlist)\n",
    "f = reduce_bond_forces_b(neighbourlist)\n",
    "f = reduce_bond_forces_c(neighbourlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd28085-164b-40d9-b9a2-07f606cc70f3",
   "metadata": {},
   "source": [
    "### Numba CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d56ea9fb-4d75-4cfa-9d73-7aa4613452de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda, float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a237755-3560-4c56-8255-4746925f13f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "\n",
    "def get_cuda_device_info(verbose=True):\n",
    "    \"\"\"\n",
    "    Retrieve comprehensive information about the current CUDA device.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    verbose : bool, optional\n",
    "        If True, print device information. If False, return as dictionary.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict or None\n",
    "        Dictionary of device properties if verbose=False, otherwise None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        device = cuda.get_current_device()\n",
    "        context = cuda.current_context()\n",
    "        cuda_version = cuda.runtime.get_version()  # (major, minor)\n",
    "\n",
    "        device_info = {\n",
    "            \"cuda_runtime_version\": f\"{cuda_version[0]}.{cuda_version[1]}\",\n",
    "            \"name\": device.name,\n",
    "            \"compute_capability\": device.compute_capability,\n",
    "            \"total_memory_gb\": context.get_memory_info().total / 1e9,\n",
    "            \"free_memory_gb\": context.get_memory_info().free / 1e9,\n",
    "            \"multiprocessors\": device.MULTIPROCESSOR_COUNT,\n",
    "            \"max_threads_per_block\": device.MAX_THREADS_PER_BLOCK,\n",
    "            \"max_grid_dimensions\": {\n",
    "                \"x\": device.MAX_GRID_DIM_X,\n",
    "                \"y\": device.MAX_GRID_DIM_Y,\n",
    "                \"z\": device.MAX_GRID_DIM_Z\n",
    "            },\n",
    "            \"warp_size\": device.WARP_SIZE,\n",
    "            \"clock_rate_khz\": device.CLOCK_RATE,\n",
    "            \"memory_clock_rate_khz\": device.MEMORY_CLOCK_RATE,\n",
    "        }\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"CUDA Device Information:\")\n",
    "            print(\"-\" * 40)\n",
    "            print(f\"{'CUDA Runtime Version:':<30} {device_info['cuda_runtime_version']}\")\n",
    "            print(f\"{'Device Name:':<30} {device_info['name']}\")\n",
    "            print(f\"{'Compute Capability:':<30} {device_info['compute_capability']}\")\n",
    "            \n",
    "            print(\"\\nMemory:\")\n",
    "            print(f\"{'Total Memory:':<30} {device_info['total_memory_gb']:.2f} GB\")\n",
    "            print(f\"{'Free Memory:':<30} {device_info['free_memory_gb']:.2f} GB\")\n",
    "            \n",
    "            print(\"\\nCompute Resources:\")\n",
    "            print(f\"{'Streaming Multiprocessors:':<30} {device_info['multiprocessors']}\")\n",
    "            print(f\"{'Max Threads per Block:':<30} {device_info['max_threads_per_block']}\")\n",
    "            \n",
    "            print(\"\\nGrid Limitations:\")\n",
    "            print(f\"{'Max Grid Dimensions X:':<30} {device_info['max_grid_dimensions']['x']}\")\n",
    "            print(f\"{'Max Grid Dimensions Y:':<30} {device_info['max_grid_dimensions']['y']}\")\n",
    "            print(f\"{'Max Grid Dimensions Z:':<30} {device_info['max_grid_dimensions']['z']}\")\n",
    "            \n",
    "            print(\"\\nAdditional Characteristics:\")\n",
    "            print(f\"{'Warp Size:':<30} {device_info['warp_size']}\")\n",
    "            print(f\"{'Clock Rate:':<30} {device_info['clock_rate_khz']/1e6:.2f} GHz\")\n",
    "            print(f\"{'Memory Clock Rate:':<30} {device_info['memory_clock_rate_khz']/1e6:.2f} GHz\")\n",
    "        \n",
    "        return device_info if not verbose else None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving CUDA device information: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fc752a6-a24f-45e5-bf65-405334955c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def row_sum_kernel(neighbourlist, output):\n",
    "    row = cuda.blockIdx.x\n",
    "    tid = cuda.threadIdx.x\n",
    "    n_cols = neighbourlist.shape[1]\n",
    "\n",
    "    # Allocate shared memory for each thread to load one value\n",
    "    sdata = cuda.shared.array(512, dtype=float32)  # Adjust size if needed\n",
    "\n",
    "    val = 0.0\n",
    "    if tid < n_cols:\n",
    "        val = neighbourlist[row, tid]\n",
    "\n",
    "    sdata[tid] = val\n",
    "    cuda.syncthreads()\n",
    "\n",
    "    s = cuda.blockDim.x // 2\n",
    "    while s > 0:\n",
    "        if tid < s and tid + s < n_cols:\n",
    "            sdata[tid] += sdata[tid + s]\n",
    "        cuda.syncthreads()\n",
    "        s //= 2\n",
    "\n",
    "    if tid == 0:\n",
    "        output[row] = sdata[0]\n",
    "\n",
    "def reduce_bond_forces_gpu(neighbourlist):\n",
    "    n_particles, n_family_members = neighbourlist.shape\n",
    "    threads_per_block = 512  # Match shared memory allocation\n",
    "    shared_mem = threads_per_block * 4  # float32: 4 bytes\n",
    "\n",
    "    d_neigh = cuda.to_device(neighbourlist.astype(np.float32))\n",
    "    d_out = cuda.device_array(n_particles, dtype=np.float32)\n",
    "\n",
    "    row_sum_kernel[n_particles, threads_per_block](d_neigh, d_out)\n",
    "    return d_out.copy_to_host()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc9f08a1-cf9f-48a3-a62b-aa42eb14836e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error retrieving CUDA device information: Error at driver init: \n",
      "\n",
      "CUDA driver library cannot be found.\n",
      "If you are sure that a CUDA driver is installed,\n",
      "try setting environment variable NUMBA_CUDA_DRIVER\n",
      "with the file path of the CUDA driver shared library.\n",
      ":\n"
     ]
    }
   ],
   "source": [
    "get_cuda_device_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ec3189a-d455-4489-baaf-ea388669f8db",
   "metadata": {},
   "outputs": [
    {
     "ename": "CudaSupportError",
     "evalue": "Error at driver init: \n\nCUDA driver library cannot be found.\nIf you are sure that a CUDA driver is installed,\ntry setting environment variable NUMBA_CUDA_DRIVER\nwith the file path of the CUDA driver shared library.\n:",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCudaSupportError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m f = \u001b[43mreduce_bond_forces_gpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneighbourlist\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mreduce_bond_forces_gpu\u001b[39m\u001b[34m(neighbourlist)\u001b[39m\n\u001b[32m     29\u001b[39m threads_per_block = \u001b[32m512\u001b[39m  \u001b[38;5;66;03m# Match shared memory allocation\u001b[39;00m\n\u001b[32m     30\u001b[39m shared_mem = threads_per_block * \u001b[32m4\u001b[39m  \u001b[38;5;66;03m# float32: 4 bytes\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m d_neigh = \u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneighbourlist\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m d_out = cuda.device_array(n_particles, dtype=np.float32)\n\u001b[32m     35\u001b[39m row_sum_kernel[n_particles, threads_per_block](d_neigh, d_out)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/virtualenvs/cuda-z6WtAaew/lib/python3.11/site-packages/numba/cuda/cudadrv/devices.py:231\u001b[39m, in \u001b[36mrequire_context.<locals>._require_cuda_context\u001b[39m\u001b[34m(*args, **kws)\u001b[39m\n\u001b[32m    229\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(fn)\n\u001b[32m    230\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_require_cuda_context\u001b[39m(*args, **kws):\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_runtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43mensure_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mreturn\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkws\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.11/3.11.6/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py:137\u001b[39m, in \u001b[36m_GeneratorContextManager.__enter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args, \u001b[38;5;28mself\u001b[39m.kwds, \u001b[38;5;28mself\u001b[39m.func\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m.gen)\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mgenerator didn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt yield\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/virtualenvs/cuda-z6WtAaew/lib/python3.11/site-packages/numba/cuda/cudadrv/devices.py:121\u001b[39m, in \u001b[36m_Runtime.ensure_context\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;129m@contextmanager\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mensure_context\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    112\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Ensure a CUDA context is available inside the context.\u001b[39;00m\n\u001b[32m    113\u001b[39m \n\u001b[32m    114\u001b[39m \u001b[33;03m    On entrance, queries the CUDA driver for an active CUDA context and\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    119\u001b[39m \u001b[33;03m    any top-level Numba CUDA API.\u001b[39;00m\n\u001b[32m    120\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_active_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m        \u001b[49m\u001b[43moldctx\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_attached_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnewctx\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_or_create_context\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/virtualenvs/cuda-z6WtAaew/lib/python3.11/site-packages/numba/cuda/cudadrv/driver.py:495\u001b[39m, in \u001b[36m_ActiveContext.__enter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    494\u001b[39m     hctx = drvapi.cu_context(\u001b[32m0\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m495\u001b[39m     \u001b[43mdriver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuCtxGetCurrent\u001b[49m(byref(hctx))\n\u001b[32m    496\u001b[39m     hctx = hctx \u001b[38;5;28;01mif\u001b[39;00m hctx.value \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    498\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hctx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/virtualenvs/cuda-z6WtAaew/lib/python3.11/site-packages/numba/cuda/cudadrv/driver.py:295\u001b[39m, in \u001b[36mDriver.__getattr__\u001b[39m\u001b[34m(self, fname)\u001b[39m\n\u001b[32m    292\u001b[39m \u001b[38;5;28mself\u001b[39m.ensure_initialized()\n\u001b[32m    294\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.initialization_error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CudaSupportError(\u001b[33m\"\u001b[39m\u001b[33mError at driver init: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m:\u001b[39m\u001b[33m\"\u001b[39m %\n\u001b[32m    296\u001b[39m                            \u001b[38;5;28mself\u001b[39m.initialization_error)\n\u001b[32m    298\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m USE_NV_BINDING:\n\u001b[32m    299\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._cuda_python_wrap_fn(fname)\n",
      "\u001b[31mCudaSupportError\u001b[39m: Error at driver init: \n\nCUDA driver library cannot be found.\nIf you are sure that a CUDA driver is installed,\ntry setting environment variable NUMBA_CUDA_DRIVER\nwith the file path of the CUDA driver shared library.\n:"
     ]
    }
   ],
   "source": [
    "f = reduce_bond_forces_gpu(neighbourlist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
