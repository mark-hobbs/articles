{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b499b0e7-b267-44bf-962c-43df3924a81e",
   "metadata": {},
   "source": [
    "## Metropolis-Hastings\n",
    "\n",
    "When the search space becomes larger, or the model is expensive to evaluate, it can become infeasible to do an exhaustive search and we must turn to randomised searches. Markov Chain Monte Carlo (MCMC) methods are the most common approach in such scenarios. The aim of MCMC is to randomly walk through the parameter space, while the fraction of time spent at each state $\\theta_i$ is $\\propto$ the unormalised posterior. While various libraries exist for posterior exploration, we will implement a [Metropolis-Hastings](https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm) sampler to aid comprehension. Please note that the code for the sampler below is originally sourced from this [blog](https://colindcarroll.com/2018/11/24/animated-mcmc-with-matplotlib/) by Colin Carroll.\n",
    "\n",
    "In the next [example](02-linear-elasticity-perfect.ipynb) we will write a `Sampler` class\n",
    "\n",
    "### Statistical summaries\n",
    "\n",
    "Once the posterior has been sampled, it needs to be analysed to determine the statistical summaries.\n",
    "\n",
    "During the initial sampling phase (commonly referred to as the burn-in phase), the chain is still adjusting and exploring the parameter space, potentially starting from an arbitrary initial state or distribution. The initial samples may not accurately represent the true posterior distribution because the chain has not yet stabilised. Therefore, these initial samples are discarded (burned) and not used for analysis purposes.\n",
    "\n",
    "After discarding the burn-in samples, the remaining samples can be used to estimate various statistical summaries. One key summary is the **Maximum A Posteriori (MAP) estimate**, which is the mode of the posterior distribution and represents the most likely value given the data. Additionally, **credible intervals** can be computed to provide a range of values within which the true parameter value is likely to lie with a certain probability.\n",
    "\n",
    "### MCMC convergence\n",
    "\n",
    "Assessing convergence in Markov Chain Monte Carlo (MCMC) methods is crucial to ensure that the generated samples adequately represent the target distribution. Several methods can be used to assess convergence:\n",
    "\n",
    "1. **Visual Inspection**: Plotting trace plots for each parameter can give a visual indication of convergence. Trace plots should appear as random walks without clear trends or patterns.\n",
    "\n",
    "2. **Gelman-Rubin Diagnostic (R-hat)**: This diagnostic compares the variance between multiple chains to the variance within each chain. An R-hat close to 1 indicates convergence. Typically, an R-hat value less than 1.1 is considered acceptable.\n",
    "\n",
    "3. **Effective Sample Size (ESS)**: ESS estimates the number of independent samples generated by the MCMC chain. It accounts for autocorrelation within the chain. Higher ESS indicates better mixing and convergence.\n",
    "\n",
    "4. **Autocorrelation**: Autocorrelation measures the correlation between a sample and lagged versions of itself. High autocorrelation indicates poor mixing. Plotting autocorrelation functions can help visualize this.\n",
    "\n",
    "5. **Geweke Diagnostic**: This diagnostic compares the mean and variance of segments from the beginning and end of a single chain. Large discrepancies suggest lack of convergence.\n",
    "\n",
    "6. **Convergence Diagnostics Plots**: Various diagnostic plots like density plots, histogram plots, and scatter plots comparing different chains can also be helpful in assessing convergence.\n",
    "\n",
    "7. **Bayesian p-values**: These are based on the distribution of a discrepancy measure between two halves of the chain. Large p-values suggest convergence.\n",
    "\n",
    "8. **Energy Plots**: These plots track the energy of the system over iterations. Stable energy levels indicate convergence.\n",
    "\n",
    "9. **Posterior Predictive Checks (PPCs)**: Simulate new data from the fitted model and compare it to observed data. Poor fit suggests lack of convergence.\n",
    "\n",
    "10. **Cross-Validation**: Split the data into training and test sets, fit the model on training data, and validate on test data. Consistent predictions indicate convergence.\n",
    "\n",
    "By employing a combination of these methods, you can effectively assess convergence in MCMC simulations and ensure reliable inference from the sampled distributions.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
